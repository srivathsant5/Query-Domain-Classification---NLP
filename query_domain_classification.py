# -*- coding: utf-8 -*-
"""Query Domain Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PmDEbJFAXNW-5X1EkAayF8EUdiny2OL3

# ***Query Domain Classification***

Importing required Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
#importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV 
from sklearn.metrics import f1_score,precision_score, multilabel_confusion_matrix, accuracy_score,jaccard_score, recall_score, hamming_loss,confusion_matrix
from sklearn.multiclass import OneVsRestClassifier

from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from lightgbm import LGBMClassifier
from sklearn.linear_model import SGDClassifier

import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

"""Loading the data"""

df = pd.read_csv('train.csv')
df.head()

#lets check the shape of the data 
df.shape

#lets check the data info
df.info()

"""Lets see how the Query  look like"""

df['Title'][0]

#Create a new subset of the data by only taking the 2nd column onwards (comments and categories)
#data_count = df.iloc[:,1:].sum()

#data_count

"""Feature Engineering

Text processing
"""

#importing required libraries for text processing 
import nltk
import re
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import wordnet
from nltk.corpus import wordnet as wn
#import lemmatizer

#function for lemmatization
def lemm(text):
    lemmatizer = wordnet.WordNetLemmatizer()
    word_tokens = word_tokenize(text)
    lemmas = [lemmatizer.lemmatize(word, wn.VERB) for word in word_tokens]
    
    return " ".join(lemmas)
#function for removing stopwords    
def rem_stopwords(text):
    stop_words = set(stopwords.words('english'))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word not in string.punctuation]
    filtered_text = [word for word in filtered_text if word not in stop_words]
    return " ".join(filtered_text)

def decontracted(text):
    text = re.sub(r"won't", "will not", text)
    text = re.sub(r"can\'t", "can not", text)
    text = re.sub(r"im", "i am", text)
    text = re.sub(r"yo", "you",text)
    text = re.sub(r"youu", "you",text)
    text = re.sub(r"n\'t", " not", text)
    text = re.sub(r"\'re", " are", text)
    text = re.sub(r"\'s", " is", text)
    text = re.sub(r"\'d", " would", text)
    text = re.sub(r"\'ll", " will", text)
    text = re.sub(r"\'t", " not", text)
    text = re.sub(r"\'ve", " have", text)
    text = re.sub(r"\'m", " am", text)
    text = re.sub(r'http\S+', '', text) #removing urls
    return text

df.head()

df.isna().sum()

df = df.dropna()

df.isna().sum()

df.dtypes

#  df['Title'] = df.Title.apply(lambda x: x.lower()) #lowering all text

def text_proc(df):
    
    df['Title'] = df.Title.apply(lambda x: x.lower()) #lowering all text
    df['Title'] = df.Title.apply(lambda x: re.sub(r'\d+','',x)) #removing numbers
    df['Title'] = df.Title.apply(lambda x: re.sub(r'\n',' ',x)) #removing \n
    df['Title'] = df.Title.apply(lambda x: decontracted(x))
    df['Title'] = df.Title.apply(lambda x: lemm(x))
    
    #removing punctuations
    translator = str.maketrans('','', string.punctuation)
    df['Title'] = df.Title.apply(lambda x : x.translate(translator))
    df['Title'] = df.Title.apply(lambda x: rem_stopwords(x))
text_proc(df)

#import nltk
#nltk.download('all')

#lets have a look at data set now
df

"""EDA"""

import wordcloud
from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

def wordcloud(df, label):
    
    # Print only rows where the toxic category label value is 1 (ie. the comment is toxic)
    subset=df[df[label]==1]
    text=subset.Title.values
    wc= WordCloud(background_color="white",max_words=4000)

    wc.generate(" ".join(text))

    plt.figure(figsize=(20,20))
    plt.subplot(221)
    plt.axis("off")
    plt.title("Words frequented in {}".format(label), fontsize=20)
    plt.imshow(wc.recolor(colormap= 'gist_earth' , random_state=244))

#df_mal = df.loc[:,['ID','Title','Domain']]

#wordcloud(df, 'Domain')

"""Model building"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.multiclass import OneVsRestClassifier

tfidf = TfidfVectorizer(analyzer='word',max_features = 2000)

x = tfidf.fit_transform(df.Title)

x.shape

y = df.Domain
print(y.value_counts())
from sklearn.preprocessing import LabelEncoder
# Create a label encoder object
label_encoder = LabelEncoder()

# Fit the encoder on the target column and transform it
y = label_encoder.fit_transform(y)

np.unique(y, return_counts=True)

x

y

tfidf.vocabulary_

#lets check the shape of x and y
x.shape, y.shape

"""## **Spliting data into train and test sets**"""

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25, random_state = 42)

#lets define different algorithms
svc = LinearSVC()
lr = LogisticRegression(solver='lbfgs')
mnb = MultinomialNB()
lgb = LGBMClassifier()
sgd = SGDClassifier()

#function for printing score
def print_score(y_pred,clf):
    print('classifier:',clf.__class__.__name__)
    #print("Jaccard score: {}".format(jaccard_score(y_test,y_pred,average='micro')))
    print("Accuracy score: {}".format(accuracy_score(y_test,y_pred)))
    print("f1_score: {}".format(f1_score(y_test,y_pred,average='micro')))
    print("Precision : ", precision_score(y_test,y_pred,average='micro'))
    print("Recall: {}".format(recall_score(y_test,y_pred,average='micro')))
   # print("Hamming loss: ", hamming_loss(y_test,y_pred))
    print("Confusion matrix:\n ", confusion_matrix(y_test,y_pred))
    print('========================================\n')

#models with evaluation using OneVsRestClassifier
for classifier in [svc,lr,mnb,sgd,lgb]:
   classifier.fit(x_train,y_train)
   y_pred = classifier.predict(x_test)
   print_score(y_pred, classifier)

"""Hyperparameter Tuning"""

param = {
        'penalty': ['l1'],
        'loss': ['hinge','squared_hinge'],
        'multi_class': ['ovr','crammer_singer'],
        'dual': [False],
        'intercept_scaling': [2,4,5],
        'C': [2]
        }

#train the model with given parameters using GridSearchCV
svc = LinearSVC()
GCV =  GridSearchCV(svc,param,cv = 3, verbose =2,n_jobs=-1)
GCV.fit(x_train,y_train)

GCV.best_params_

"""# Final model"""

model = LinearSVC(C=2,dual = False, loss='hinge',multi_class='crammer_singer', penalty ='l1',intercept_scaling=2)
model.fit(x_train,y_train)
y_pred = model.predict(x_test)

print("Jaccard score: {}".format(jaccard_score(y_test,y_pred,average='micro')))
print("Accuracy score: {}".format(accuracy_score(y_test,y_pred)))
print("f1_score: {}".format(f1_score(y_test,y_pred,average='micro')))
print("Precision : ", precision_score(y_test,y_pred,average='micro'))
print("Recall: {}".format(recall_score(y_test,y_pred,average='micro')))
#print("Hamming loss: ", hamming_loss(y_test,y_pred))
print("\nConfusion matrix: \n", confusion_matrix(y_test,y_pred))

model.predict(x_test)

"""# Prediction for test dataset using final model"""

#Lets load the test data set
test = pd.read_csv('test.csv')
test.head()

test.info()

test.isna().sum()

text_proc(test)

tfidf = TfidfVectorizer(analyzer='word',max_features = 2000)
x1 = tfidf.fit_transform(test.Title)

x1

x1.shape

#lets predict the output
model.predict(x1)

predictions = model.predict(x1)

pred=pd.DataFrame(predictions, columns = ['Target'])
pred

x_test = pd.concat([test,pred], axis=1)
x_test

x_test.Target.value_counts()

x_test.dtypes

x_test['Domain'] = x_test['Target'].replace({0: 'Career' ,
                                             1: 'Hackathons',
                                             2:'Misc',
                                             3: 'Other',
                                             4:'Resources',
                                             5: 'Techniques',
                                             6: 'Tools'})

x_test

x_test.drop(columns = ['Title','Target'],inplace = True)

x_test

#saving the data into csv file
x_test.to_csv(r"solution_submission.csv")

x_test

